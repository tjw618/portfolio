import os
import re
import pickle
import pandas as pd
import win32com.client
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from collections import defaultdict

# ===== åŸºæœ¬è¨­å®š =====
fit_folder = "åˆé©å±¥æ­·"
unfit_folder = "ä¸åˆé©å±¥æ­·"
excel_path = "å±¥æ­·ç¯©é¸.xlsx"
output_path = excel_path
time_file = "lasttime.txt"

os.makedirs(fit_folder, exist_ok=True)
os.makedirs(unfit_folder, exist_ok=True)

# ===== æ™‚é–“å·¥å…· =====
def get_last_processed_time():
    if os.path.exists(time_file):
        with open(time_file, "r") as f:
            ts = f.read().strip()
            return datetime.strptime(ts, "%Y-%m-%d %H:%M:%S")
    else:
        return datetime.now() - timedelta(days=60)

def save_current_time():
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(time_file, "w") as f:
        f.write(now)

# ===== ç²—ç¯©æ¢ä»¶ / å­¸æ­· bucket =====
def is_fit_candidate(text):
    accepted_genders = ["ç”·", "å¥³"]
    accepted_degrees = ["ç¢©å£«", "åšå£«", "å¤§å­¸"]
    min_age = 20
    max_age = 35

    age_match = re.search(r"(\d{1,2})æ­²", text)
    gender_match = re.search(r"(ç”·|å¥³)", text)
    degree_match = re.search(r"(åœ‹ä¸­\(å«\)ä»¥ä¸‹|é«˜ä¸­|é«˜è·|äº”å°ˆ|ä¸‰å°ˆ|äºŒå°ˆ|äºŒæŠ€|å››æŠ€|å¤§å­¸|ç¢©å£«|åšå£«)", text)
    age = int(age_match.group(1)) if age_match else 99
    gender = gender_match.group(1) if gender_match else ""
    degree = degree_match.group(1) if degree_match else ""

    return (
        (min_age <= age <= max_age) and
        (gender in accepted_genders) and
        (degree in accepted_degrees)
    )

def degree_to_bucket(degree_str: str) -> str:
    if not degree_str:
        return "å…¶ä»–"
    m = re.search(r"(åœ‹ä¸­\(å«\)ä»¥ä¸‹|é«˜ä¸­|é«˜è·|äº”å°ˆ|ä¸‰å°ˆ|äºŒå°ˆ|äºŒæŠ€|å››æŠ€|å¤§å­¸|ç¢©å£«|åšå£«)", str(degree_str))
    base = m.group(1) if m else ""
    if base in {"é«˜ä¸­", "é«˜è·"}:
        return "é«˜ä¸­è·"
    if base in {"äºŒå°ˆ", "ä¸‰å°ˆ", "äº”å°ˆ"}:
        return "å°ˆç§‘"
    if base in {"äºŒæŠ€", "å››æŠ€", "å¤§å­¸"}:
        return "å¤§å­¸"
    if base in {"ç¢©å£«"}:
        return "ç¢©å£«"
    if base in {"åšå£«"}:
        return "åšå£«"
    return "å…¶ä»–"

# ===== æ–‡å­—è™•ç† / å­¸æ ¡ç§‘ç³»è§£æï¼šæ­£å‰‡èˆ‡å·¥å…· =====
_SCHOOL_RE = re.compile(
    r"(?:" 
    r"å¤§å­¸|ç§‘æŠ€å¤§å­¸|ç§‘å¤§|å¤§å­¸æ ¡é™¢|å°ˆç§‘å­¸æ ¡|æŠ€è¡“å­¸é™¢|å­¸é™¢|å­¸æ ¡|é«˜ä¸­|é«˜è·|ä¸­å­¸|å•†å·¥|å®¶å•†|é«˜å·¥|åœ‹ä¸­|ç©ºä¸­å¤§å­¸"
    r"|University|College|Institute(?: of Technology)?|Polytechnic|Academy|Conservatory"
    r"|Business School|Law School|Medical School|Graduate School"
    r"|High School|Secondary School|school|Secondary|Junior College|Community College"
    r")",
    re.I
)

_MAJOR_RE  = re.compile(
    r"(?:"  # ç§‘ç³»é—œéµè©ï¼ˆä¸­è‹±ï¼‰
    r"å­¸ä½å­¸ç¨‹|å­¸å£«ç­|ç¢©å£«ç­|åšå£«ç­|å­¸ç³»|ç³»æ‰€|ç³»|ç§‘|æ‰€|å­¸ç¨‹|ç ”ç©¶æ‰€|å­¸é™¢|æ—¥é–“éƒ¨|å¤œé–“éƒ¨|é€²ä¿®éƒ¨|åœ¨è·å°ˆç­|å­¸ç¾¤|å­¸éƒ¨"
    r"|Department of|School of|Program in|Graduate Institute of|MBA|EMBA|M\.?S\.?|Ph\.?D\.?"
    r")",
    re.I
)

# æ—¥æœŸï¼æœŸé–“æ¨£å¼ï¼ˆçµ¦æ•™è‚²èƒŒæ™¯ç¯©æ‰æ—¥æœŸæ¬„ä½ç”¨ï¼‰
DATE_LIKE = re.compile(
    r"(\d{2,4}[./-]\d{1,2}(?:[./-]\d{1,2})?|æ°‘åœ‹\s*\d{2,3}\s*å¹´|\d{2,4}å¹´|\d{1,2}æœˆ|è‡³|~|èµ·|æ­¢|å­¸å¹´åº¦|æœŸé–“)"
)
PROGRAM_MAJOR_RE = re.compile(
    r'\b(?:EMBA|E[-\s]?MBA|Executive\s+MBA|MBA|M\.?S\.?|MSc|MSF|MFin|M\.?A\.?|MA|'
    r'M\.?Eng\.?|MEng|MPA|MPP|MPH|MAcc|LL\.?M\.?|LLM|J\.?D\.?|JD|M\.?D\.?|MD|'
    r'Ph\.?D\.?|DPhil|DBA|EdD|DVM|DDS)\b',
    re.I
)

def _move_program_tokens_from_school_to_major(school: str, major: str) -> tuple[str, str]:
    if not school:
        return school, (major or "")
    tokens = PROGRAM_MAJOR_RE.findall(school)
    if tokens:
        # å¾å­¸æ ¡ç§»é™¤å­¸ä½è©
        school = PROGRAM_MAJOR_RE.sub("", school)
        # æ¸…æ‰å› ç§»é™¤ç•™ä¸‹çš„å¤šç©ºç™½èˆ‡ç©ºæ‹¬è™Ÿ
        school = re.sub(r"\(\s*\)", "", school)
        school = re.sub(r"\s{2,}", " ", school).strip()
        # åŠ åˆ°ç§‘ç³»ï¼ˆé¿å…é‡è¤‡ï¼‰
        for t in tokens:
            if not re.search(fr'\b{re.escape(t)}\b', major or "", flags=re.I):
                major = (major + " " + t).strip()
    major = re.sub(r"\s{2,}", " ", (major or "")).strip()
    return school, major

def _looks_like_school(s: str) -> bool:
    return bool(_SCHOOL_RE.search(s or ""))

def _looks_like_major(s: str) -> bool:
    return bool(_MAJOR_RE.search(s or ""))

def _compact_zh_spaces(s: str) -> str:
    if not s:
        return s
    s = re.sub(r"(?<=[\u4e00-\u9fff])\s+(?=[\u4e00-\u9fff])", "", s)
    s = re.sub(r"^(åœ‹ç«‹|ç§ç«‹)\s+", r"\1", s)
    s = re.sub(r"\s+(ç§‘æŠ€å¤§å­¸|å¸«ç¯„å¤§å­¸|é†«å­¸å¤§å­¸|äº¤é€šå¤§å­¸|æ¸…è¯å¤§å­¸|æµ·æ´‹å¤§å­¸|é«”è‚²å¤§å­¸|è—è¡“å¤§å­¸|ä¸­é†«è—¥å¤§å­¸|ç®¡ç†å­¸é™¢)$", r"\1", s)
    return s.strip()

def strip_period_parens(s: str) -> str:
    if not s:
        return s

    PAREN  = re.compile(r"[ï¼ˆ(]([^ï¼‰)]*)[ï¼‰)]")
    KEEP   = re.compile(r"(åœ¨è·|æ—¥é–“|å¤œé–“|å‡æ—¥|é€²ä¿®|å°ˆç­|å­¸å£«ç­|ç¢©å£«ç­|åšå£«ç­|EMBA|MBA)", re.I)
    CREDIT = re.compile(r"(ç¢©å£«å­¸åˆ†|å­¸åˆ†ç­|å­¸åˆ†èª²ç¨‹|å­¸åˆ†)", re.I)
    DATE   = re.compile(r"(\d{2,4}\s*[./-å¹´/]\s*\d{1,2}(?:\s*[./-æœˆ/]\s*\d{1,2})?|æ°‘åœ‹\d{2,3}å¹´|\d{1,2}æœˆ|è‡³|~|å­¸å¹´åº¦|æœŸé–“)")
    END    = re.compile(r"(ç•¢æ¥­|è‚„æ¥­)")
    # ä½ç½®/åœ°åæ¨£å¼ï¼ˆåƒ…å­—æ¯æˆ–åƒ…ä¸­æ–‡å­—ã€é•·åº¦åˆç†ï¼‰
    LOC    = re.compile(r"^(?:[A-Za-z .,'-]{2,30}|[\u4e00-\u9fff]{1,6})$")

    def _repl(m):
        content = re.sub(r"\s+", "", m.group(1))

        # åªæœ‰å°±è®€ / åœ¨å­¸ â†’ å»æ‹¬è™Ÿ
        if re.fullmatch(r"(å°±è®€|åœ¨å­¸)", content):
            return ""
        has_keep   = bool(KEEP.search(content))
        has_credit = bool(CREDIT.search(content))
        has_date   = bool(DATE.search(content))
        has_end    = bool(END.search(content))

        # ç´”æ—¥æœŸ/ç•¢æ¥­ â†’ æ‹¿æ‰
        if (has_date or has_end) and not (has_keep or has_credit):
            return ""
        # åœ°åæ¨£å¼ â†’ åªæœ‰åœ¨ã€Œä¸æ˜¯å­¸åˆ†/å­¸åˆ†ç­/å­¸åˆ†èª²ç¨‹ï¼Œä¹Ÿä¸æ˜¯ KEEP/æ—¥æœŸ/ç•¢æ¥­ã€æ™‚æ‰æ‹¿æ‰
        if LOC.match(content) and not (has_keep or has_credit or has_date or has_end):
            return ""
        # å°±è®€/åœ¨å­¸ + æ—¥æœŸ â†’ ç°¡åŒ–
        if has_date and re.search(r"(å°±è®€|åœ¨å­¸)", content):
            return "(å°±è®€ä¸­)"
        # æœ‰ä¿ç•™é—œéµè© â†’ ä¿ç•™ï¼ˆå­¸åˆ†é¡ä¹Ÿä¸€èµ·ä¿ç•™ï¼‰
        if has_keep:
            return f"({KEEP.search(content).group(0)})"
        if has_credit:
            return f"({CREDIT.search(content).group(0)})"
        # å…¶ä»–æœªçŸ¥å…§å®¹ â†’ ä¿ç•™åŸæ¨£
        return m.group(0)

    return PAREN.sub(_repl, s).strip()

def node_text_no_cjk_space(node) -> str:
    s = "".join(node.stripped_strings).replace("\xa0", " ")
    s = re.sub(r"(?<=[\u4e00-\u9fff])\s+(?=[\u4e00-\u9fff])", "", s)
    s = re.sub(r"\s{2,}", " ", s)
    return s.strip()

def parse_school_major(line: str):
    if not line:
        return "", ""

    raw = str(line)
    raw = re.sub(r"[\u3000\xa0]+", " ", raw)
    raw = re.sub(r"[ï¼/ã€,ï¼Œ\-\â€“\â€”\|â”‚â€¢â€§ãƒ»ï¼\.]+", " ", raw)
    raw = re.sub(r"\s{2,}", " ", raw).strip()

    # å…ˆæŠŠæ‰€æœ‰æ‹¬è™Ÿåšã€Œç°¡åŒ–/å»é™¤ã€å¾Œï¼Œå†ä¾†åˆ‡ï¼ˆæœƒæ‹¿æ‰ (å°±è®€)ã€(è‹±åœ‹) ç­‰ï¼‰
    def _strip_all_parens_keep_space(s: str) -> str:
        out = strip_period_parens(s)
        return re.sub(r"\s{2,}", " ", out).strip()

    simplified = _strip_all_parens_keep_space(raw)

    # è‹±æ–‡å­¸æ ¡å°¾å­—ï¼ˆæ“´å…… Business School / Secondaryï¼‰
    en_school_tail = (
        r"(?:University|Institute of Technology|Institute|Polytechnic|College|Academy|Conservatory|"
        r"Business School|Law School|Medical School|Graduate School|"
        r"High School|Secondary School|Secondary|Junior College|Community College)"
    )

    # e.g., "Henley Business School Marketing and International Management"
    m_en = re.match(rf"^(.*?\b{en_school_tail}\b)\s+(.*)$", simplified, flags=re.I)
    if m_en:
        school = m_en.group(1).strip()
        major  = m_en.group(2).strip()

        # EMBA/MBA è¨Šè™Ÿï¼ˆä¿éšªå†è£œä¸€æ¬¡ï¼‰
        if re.search(r"\bEMBA\b", simplified, flags=re.I) and "EMBA" not in major:
            major = (major + " EMBA").strip()
        if re.search(r"\bMBA\b",  simplified, flags=re.I) and not re.search(r"\bEMBA\b", simplified, flags=re.I) and "MBA" not in major:
            major = (major + " MBA").strip()

        # ã€Œç¢©å£«å­¸åˆ†/å­¸åˆ†ç­/å­¸åˆ†èª²ç¨‹ã€â†’ æ­¸å­¸æ ¡æ¬„
        m_credit = re.search(r"(ç¢©å£«å­¸åˆ†|å­¸åˆ†ç­|å­¸åˆ†èª²ç¨‹)", simplified)
        if m_credit and m_credit.group(1) not in school:
            school = f"{school}ï¼ˆ{m_credit.group(1)}ï¼‰"

        # æ¨™æº–åŒ–å¤§å°å¯«ï¼ˆBusiness School ç­‰ï¼‰
        school = re.sub(r"\b(Business|Law|Medical|Graduate)\s+school\b", lambda m: m.group(0).title(), school, flags=re.I)

        # â€”â€” é—œéµï¼šæŠŠ EMBA/MBA/MS/PhDâ€¦ å¾å­¸æ ¡æ¬å»ç§‘ç³» â€”â€” #
        school, major = _move_program_tokens_from_school_to_major(school, major)

        school = _compact_zh_spaces(strip_period_parens(school))
        major  = _compact_zh_spaces(strip_period_parens(major))
        return school, major

    # â€”â€” ä¸­æ–‡/æ··åˆæƒ…æ³ â€”â€” #
    clean = simplified
    major_tail = r"(?:å­¸ä½å­¸ç¨‹|å­¸å£«ç­|ç¢©å£«ç­|åšå£«ç­|å­¸ç³»|ç³»æ‰€|ç³»|ç§‘|æ‰€|å­¸ç¨‹|ç ”ç©¶æ‰€|å­¸é™¢|æ—¥é–“éƒ¨|å¤œé–“éƒ¨|é€²ä¿®éƒ¨|åœ¨è·å°ˆç­)"
    m = re.match(rf"^(.*?)\s+(.+?{major_tail})$", clean) or re.match(rf"^(.+?)(.+?{major_tail})$", clean)
    if m:
        school = m.group(1).strip()
        major  = m.group(2).strip()
    else:
        # å…è¨±ã€Œå­¸æ ¡å°¾å­—ã€å¾Œé¢**æ²’æœ‰ç©ºç™½**ä¹Ÿèƒ½åˆ‡ï¼ˆé‡é»æ”¹é€™è¡Œï¼‰
        school_tail = (
            r"(?:ç§‘æŠ€å¤§å­¸|å¸«ç¯„å¤§å­¸|é†«å­¸å¤§å­¸|é«”è‚²å¤§å­¸|è—è¡“å¤§å­¸|äº¤é€šå¤§å­¸|æ¸…è¯å¤§å­¸|ä¸­é†«è—¥å¤§å­¸|æµ·æ´‹å¤§å­¸|è­¦å¯Ÿå¤§å­¸|"
            r"å¤§å­¸|æŠ€è¡“å­¸é™¢|å°ˆç§‘å­¸æ ¡|å­¸æ ¡|é«˜ä¸­|é«˜è·|ä¸­å­¸|å•†å·¥|å®¶å•†|å·¥ç ”é™¢|é«˜å·¥|"
            r"University|College|Institute(?: of Technology)?|Polytechnic|Academy|Conservatory|BCIT|"
            r"Business School|Law School|Medical School|Graduate School|"
            r"High School|Secondary School|Secondary|Junior College|Community College)"
        )
        m2 = re.match(rf"^(.*?{school_tail})(.*)$", clean, flags=re.I)  # â† æ–°ï¼šå…è¨±é›¶ç©ºç™½
        if m2:
            school = m2.group(1).strip()
            major  = (m2.group(2) or "").strip()
        else:
            m3 = re.match(r"^(.*?(é«˜ä¸­|é«˜è·|å•†å·¥|å®¶å•†|é«˜å·¥|ä¸­å­¸))\s*(.+ç§‘)?$", clean)
            if m3:
                school = m3.group(1).strip()
                major  = (m3.group(3) or "").strip()
            else:
                school, major = clean, ""

    # EMBA/MBA è¨Šè™Ÿï¼ˆä¿éšªå†è£œä¸€æ¬¡ï¼‰
    if re.search(r"\bEMBA\b", simplified, flags=re.I) and "EMBA" not in major:
        major = (major + " EMBA").strip()
    if re.search(r"\bMBA\b", simplified, flags=re.I) and not re.search(r"\bEMBA\b", simplified, flags=re.I) and "MBA" not in major:
        major = (major + " MBA").strip()

    # ã€Œç¢©å£«å­¸åˆ†/å­¸åˆ†ç­/å­¸åˆ†èª²ç¨‹ã€â†’ å­¸æ ¡
    m_credit = re.search(r"(ç¢©å£«å­¸åˆ†|å­¸åˆ†ç­|å­¸åˆ†èª²ç¨‹)", simplified)
    if m_credit:
        tag = m_credit.group(1)
        if tag in major:
            major = major.replace(tag, "").strip()
        if tag not in school:
            school = re.sub(r"\s{2,}", " ", f"{school} {tag}").strip()

    # â€”â€” é—œéµï¼šæŠŠ EMBA/MBA/MS/PhDâ€¦ å¾å­¸æ ¡æ¬å»ç§‘ç³» â€”â€” #
    school, major = _move_program_tokens_from_school_to_major(school, major)

    return _compact_zh_spaces(strip_period_parens(school)), _compact_zh_spaces(strip_period_parens(major))

# ===== èªæ–‡èƒ½åŠ›å€å¡Š + è­‰ç…§æˆç¸¾ =====
def get_lang_section(full_text: str) -> str:
    text = re.sub(r"[\u3000\xa0]+", " ", full_text)
    m = re.search(r"èªæ–‡èƒ½åŠ›\s*[:ï¼š]?", text)
    if not m:
        return ""
    start = m.end()
    nxt = re.search(
        r"(æŠ€èƒ½å°ˆé•·|æ•™è‚²èƒŒæ™¯|è‡ªå‚³|å…¶ä»–æ“…é•·å·¥å…·|å€‹äººè³‡æ–™|æ±‚è·è€…å¸Œæœ›æ¢ä»¶|å·¥ä½œç¶“æ­·|å°ˆé•·|è­‰ç…§|å°ˆæ¥­è­‰ç…§|é›»è…¦æŠ€èƒ½|å°ˆæ¥­æŠ€èƒ½)",
        text[start:]
    )
    end = start + (nxt.start() if nxt else 4000)
    return text[start:end]

_PROFICIENCY_WORDS = r"(?:ç²¾é€š|æµåˆ©|æ¯èª|ä¸­ä¸Š|ä¸­ç­‰|ä¸­éš|åˆéš|åˆç­‰|åŸºç¤|ç•¥æ‡‚|æ™®é€š|è‰¯å¥½|å¾…åŠ å¼·|A1|A2|B1|B2|C1|C2)"

def _clean_cert_name(name: str) -> str :
    name = re.sub(r"^(?:èªæ–‡èƒ½åŠ›|èªè¨€èƒ½åŠ›|ä¸­æ–‡|åœ‹èª|è¯èª|è‹±æ–‡|è‹±èª|æ—¥æ–‡|æ—¥èª|éŸ“æ–‡|éŸ“èª|å¾·æ–‡|å¾·èª|æ³•æ–‡|æ³•èª|è¥¿ç­ç‰™æ–‡|è¥¿èª|è¶Šæ–‡|è¶Šèª|ç²µèª|å®¢èª|å°èª)\s*[:ï¼š\-]?\s*", "", name)
    name = re.sub(rf"^(?:{_PROFICIENCY_WORDS})\s*", "", name)
    name = re.sub(rf"\b{_PROFICIENCY_WORDS}\b", "", name)
    name = re.sub(r"[ã€,ï¼Œ;ï¼›\-â€“â€”]+", " ", name)
    name = re.sub(r"\s{2,}", " ", name).strip()
    name = re.sub(r"^[ï¼ˆ(]\s*[ï¼‰)]$", "", name)
    return name

def extract_cert_scores_from_lang(lang_text: str) -> str:
    if not lang_text:
        return ""

    s = re.sub(r"[\u3000\xa0]+", " ", lang_text)
    s = re.sub(r"\s{2,}", " ", s)

    found = []
    seen = set()

    # ---- è‹±æ–‡æª¢å®šï¼ˆä¿ç•™åŸå‰‡ï¼‰----
    for m in re.finditer(r"(?:TOEIC|å¤šç›Š)[^\d]{0,8}(\d{3,4})", s, flags=re.I):
        score = m.group(1)
        k = f"toeic {score}"
        if k not in seen:
            seen.add(k); found.append(f"TOEIC {score}")

    for m in re.finditer(r"\bTOEFL(?:\s*iBT)?[^\d]{0,8}(\d{1,3})\b", s, flags=re.I):
        v = int(m.group(1))
        if 0 <= v <= 120:
            k = f"toefl ibt {v}"
            if k not in seen:
                seen.add(k); found.append(f"TOEFL iBT {v}")

    for m in re.finditer(r"\bTOEFL\s*ITP[^\d]{0,8}([3-6]\d{2}|677)\b", s, flags=re.I):
        v = int(m.group(1))
        if 310 <= v <= 677:
            k = f"toefl itp {v}"
            if k not in seen:
                seen.add(k); found.append(f"TOEFL ITP {v}")

    for m in re.finditer(r"\bIELTS[^\d]{0,8}([1-9](?:\.5|\.0)?)\b", s, flags=re.I):
        try:
            v = float(m.group(1))
            if 1.0 <= v <= 9.0:
                k = f"ielts {m.group(1)}"
                if k not in seen:
                    seen.add(k); found.append(f"IELTS {m.group(1)}")
        except:
            pass

    for m in re.finditer(r"(?:\bGEPT\b|å…¨æ°‘è‹±æª¢)\s*[:ï¼š\-]?\s*(åˆç´š|ä¸­ç´š|ä¸­é«˜ç´š|é«˜ç´š|åˆ|ä¸­|ä¸­é«˜|é«˜)\b", s, flags=re.I):
        lvl = m.group(1)
        if lvl in {"åˆ","ä¸­","ä¸­é«˜","é«˜"}:
            lvl = {"åˆ":"åˆç´š","ä¸­":"ä¸­ç´š","ä¸­é«˜":"ä¸­é«˜ç´š","é«˜":"é«˜ç´š"}[lvl]
        k = f"gept {lvl}"
        if k not in seen:
            seen.add(k); found.append(f"GEPT {lvl}")

    for m in re.finditer(r"(?:\bJLPT\b|æ—¥æª¢)\s*[:ï¼š\-]?\s*(N[1-5])\b", s, flags=re.I):
        lvl = m.group(1).upper()
        k = f"jlpt {lvl}"
        if k not in seen:
            seen.add(k); found.append(f"JLPT {lvl}")

    # ---- æ³›ç”¨ä¸­æ–‡è­‰ç…§ï¼ˆåˆ‡æ®µ â†’ å–å†’è™Ÿå¾Œ â†’ æ¯”å°ï¼‰----
    EXCLUDE_NAMES = {"ä¸­æ–‡","åœ‹èª","è¯èª","è‹±æ–‡","è‹±èª","æ—¥æ–‡","æ—¥èª","éŸ“æ–‡","éŸ“èª","å¾·æ–‡","å¾·èª",
                     "æ³•æ–‡","æ³•èª","è¥¿ç­ç‰™æ–‡","è¥¿èª","è¶Šæ–‡","è¶Šèª","ç²µèª","å®¢èª","å°èª","æ³°æ–‡","æ³°èª","å…¶ä»–å¤–æ–‡"}

    tokens = re.split(r"[ã€,ï¼Œ;ï¼›\n\r]+", s)
    LEVEL_PAT = r"(?:[A-Za-z]?\d(?:\.\d)?|[ABC][+\-]?|[Nn][1-5]|[1-9]\d{2,3}|åˆç­‰|ä¸­ç­‰|é«˜ç­‰|å„ªç­‰)"

    for t in tokens:
        if not t.strip():
            continue
        # åªçœ‹æœ€å¾Œä¸€å€‹å†’è™Ÿï¼ˆå»æ‰ã€Œä¸­æ–‡ï¼šã€ã€Œè‹±æ–‡ï¼šã€é€™é¡å‰ç¶´ï¼‰
        if "ï¼š" in t or ":" in t:
            t = re.split(r"[ï¼š:]", t)[-1].strip()

        if "è­‰ç…§" not in t and "æª¢å®š" not in t:
            continue

        m = re.search(
            rf"([ä¸€-é¾¥A-Za-z0-9ï¼ˆï¼‰()ã€Šã€‹ã€ˆã€‰\-\s]{{2,50}}?)(?:è­‰ç…§|æª¢å®š)\s*(?:[:ï¼š\-]?\s*({LEVEL_PAT}))?",
            t
        )
        if not m:
            continue

        raw_name = m.group(1)
        lvl = (m.group(2) or "").upper()

        name = _clean_cert_name(raw_name)
        if not name or name in EXCLUDE_NAMES:
            continue
        if re.fullmatch(_PROFICIENCY_WORDS, name):
            continue

        item = f"{name}è­‰ç…§" + (f" {lvl}" if lvl else "")
        k = item.lower()
        if k not in seen:
            seen.add(k); found.append(item)

    return "ã€".join(found)

# ===== æ•™è‚²èƒŒæ™¯æŠ½å– =====
def extract_education_fields_from_html(html: str):

    def looks_school(s: str) -> bool:
        return _looks_like_school(s)

    def looks_major(s: str) -> bool:
        return _looks_like_major(s)

    def is_date_like(s: str) -> bool:
        return bool(DATE_LIKE.search(s or ""))

    def degree_rank(text: str) -> int:
        if not text: return 0
        t = str(text)
        if re.search(r"åšå£«", t): return 5
        if re.search(r"ç¢©å£«", t): return 4
        if re.search(r"(å¤§å­¸|å­¸å£«|å››æŠ€)", t): return 3
        if re.search(r"(äºŒå°ˆ|ä¸‰å°ˆ|äº”å°ˆ|äºŒæŠ€)", t): return 2
        if re.search(r"(é«˜ä¸­|é«˜è·)", t): return 1
        return 0

    def normalize_degree(text: str) -> str:
        if not text: return ""
        t = re.sub(r"\s+", " ", str(text)).strip()
        m = re.search(r"(åœ‹ä¸­\(å«\)ä»¥ä¸‹|é«˜ä¸­|é«˜è·|äº”å°ˆ|ä¸‰å°ˆ|äºŒå°ˆ|äºŒæŠ€|å››æŠ€|å¤§å­¸|å­¸å£«|ç¢©å£«|åšå£«)\s*(ç•¢æ¥­|è‚„æ¥­|å°±å­¸ä¸­)?", t)
        if not m: return ""
        base = m.group(1).replace("å­¸å£«", "å¤§å­¸")
        stat = m.group(2) or ""
        return base + stat

    soup = BeautifulSoup(html, "html.parser")

    # 1) éŒ¨å®šã€Šæ•™è‚²èƒŒæ™¯ã€‹
    anchor = soup.find(string=re.compile("æ•™è‚²èƒŒæ™¯"))
    if not anchor:
        return ("", "", "", "", "")

    # 2) æ‰¾åˆ°ç¬¬ä¸€å¼µåŒæ™‚åŒ…å«ã€Œæœ€é«˜å­¸æ­·ï¼æœ€é«˜ï¼æ¬¡é«˜ã€çš„è¡¨
    STOP_RE = re.compile(r"(å€‹äººè³‡æ–™|æ±‚è·è€…å¸Œæœ›æ¢ä»¶|å·¥ä½œç¶“æ­·|æŠ€èƒ½å°ˆé•·|èªæ–‡èƒ½åŠ›|è‡ªå‚³|å…¶ä»–)")
    target_table = None
    node = anchor
    for _ in range(2000):
        node = node.find_next()
        if not node: break
        if getattr(node, "name", "") == "table":
            txt = node_text_no_cjk_space(node)
            if re.search(r"(æœ€é«˜å­¸æ­·|æœ€é«˜|æ¬¡é«˜)", txt):
                target_table = node
                break
            if STOP_RE.search(txt):
                break

    degree_text = ""
    hi_cells, se_cells = [], []

    if target_table:
        for tr in target_table.find_all("tr"):
            cells = [c.get_text(" ", strip=True) for c in tr.find_all(["th","td"])]
            if not cells:
                continue
            head = cells[0]
            if re.search(r"æœ€é«˜å­¸æ­·", head):
                degree_text = cells[-1]
            elif re.fullmatch(r"\s*æœ€é«˜\s*", head) or ("æœ€é«˜" in head and not hi_cells):
                hi_cells = cells[1:]  # å»æ‰ label
            elif re.fullmatch(r"\s*æ¬¡é«˜\s*", head) or ("æ¬¡é«˜" in head and not se_cells):
                se_cells = cells[1:]

    # â€”â€” æ ¸å¿ƒï¼šå¾å€¼æ¬„ä¸­æŒ‘å­¸æ ¡/ç§‘ç³»ï¼ˆå¿½ç•¥æ—¥æœŸé¡æ¬„ä½ï¼‰ â€”â€” #
    def pick_school_major(value_cells):
        # å…ˆæ¸…ä¹¾æ·¨ã€å»æ‰æ—¥æœŸé¡æ¬„ä½
        cells = [v.strip() for v in value_cells if v and v.strip()]
        non_date = [v for v in cells if not is_date_like(v)]

        # 1) å…ˆç”¨ parser åƒã€Œæ•´æ®µã€ï¼šåŒä¸€æ ¼åŒæ™‚æœ‰å­¸æ ¡èˆ‡ç§‘ç³»æ™‚ï¼Œé€™ä¸€æ­¥æœƒæ­£ç¢ºåˆ‡é–‹
        joined = " ".join(non_date) if non_date else " ".join(cells)
        s, m = parse_school_major(joined)
        s = _compact_zh_spaces(s)
        m = _compact_zh_spaces(m)

        # é‡å°å¶ç™¼æ®˜ç•™ï¼šè‹¥å­¸æ ¡èˆ‡ç§‘ç³»åŒæ™‚å«æœ‰ EMBA/MBAï¼Œå¾å­¸æ ¡å†æ¸…ä¸€æ¬¡
        if re.search(r"\b(?:EMBA|MBA)\b", s, flags=re.I) and re.search(r"\b(?:EMBA|MBA)\b", m, flags=re.I):
            s = re.sub(r"\b(?:EMBA|MBA)\b", "", s, flags=re.I)
            s = re.sub(r"\s{2,}", " ", s).strip()

        if s or m:
            return s, m

        # 2) Parser æ²’åƒåˆ°æ‰ç”¨èˆŠçš„ä¿åº•ï¼šA. å­¸æ ¡åœ¨å‰ + ä¸‹ä¸€æ ¼ç•¶ç§‘ç³»
        for i, v in enumerate(non_date):
            if looks_school(v) and i + 1 < len(non_date):
                s = _compact_zh_spaces(v)
                m = _compact_zh_spaces(non_date[i + 1])
                return s, m

        # 3) å†ä¸è¡Œç”¨ B. ã€Œåƒå­¸æ ¡ + åƒç§‘ç³»ã€
        school_cands = [v for v in non_date if looks_school(v)]
        major_cands  = [v for v in non_date if looks_major(v)]
        if school_cands and major_cands:
            return _compact_zh_spaces(school_cands[0]), _compact_zh_spaces(major_cands[-1])

        # 4) æœ€å¾Œä¿åº•
        return _compact_zh_spaces(joined), ""

    hi_school = hi_major = se_school = se_major = ""
    if hi_cells:
        hi_school, hi_major = pick_school_major(hi_cells)
    if se_cells:
        se_school, se_major = pick_school_major(se_cells)

    # 3) Fallbackï¼ˆä»é™ã€Šæ•™è‚²èƒŒæ™¯ã€‹å€å¡Šï¼‰ï¼šæŒ‘å­¸æ­·æœ€é«˜çš„å…©ç­†
    if not degree_text or not hi_school:
        section_tables = []
        node = anchor
        for _ in range(2000):
            node = node.find_next()
            if not node: break
            if getattr(node, "name", "") == "table":
                txt = node_text_no_cjk_space(node)
                if STOP_RE.search(txt): break
                section_tables.append(node)

        candidates = []
        for table in section_tables:
            for tr in table.find_all("tr"):
                cells = [node_text_no_cjk_space(c) for c in tr.find_all(["th","td"])]
                if not cells: continue
                if len(cells) == 1 and "æ•™è‚²èƒŒæ™¯" in cells[0]: continue
                line = " ".join(cells)
                if re.search(r"(åšå£«|ç¢©å£«|å¤§å­¸|å­¸å£«|å››æŠ€|äº”å°ˆ|ä¸‰å°ˆ|äºŒå°ˆ|äºŒæŠ€|é«˜ä¸­|é«˜è·)", line) and \
                   re.search(r"(å­¸ç³»|ç³»æ‰€|ç³»|ç§‘|æ‰€|å­¸ç¨‹|ç ”ç©¶æ‰€|å­¸é™¢)", line):
                    deg = normalize_degree(line)
                    school, major = pick_school_major(cells[1:] if len(cells)>1 else cells)
                    rk = degree_rank(deg or line)
                    if school:
                        candidates.append({"degree": deg, "school": school, "major": major, "rank": rk})

        if candidates:
            candidates.sort(key=lambda x: (-x["rank"]))
            if not degree_text:
                degree_text = candidates[0]["degree"]
            if not hi_school:
                hi_school, hi_major = candidates[0]["school"], candidates[0]["major"]
            if len(candidates) >= 2 and not se_school:
                se_school, se_major = candidates[1]["school"], candidates[1]["major"]

    degree_text = re.sub(r"\s+", " ", degree_text or "").strip()
    hi_school = strip_period_parens(_compact_zh_spaces(hi_school))
    hi_major  = strip_period_parens(_compact_zh_spaces(hi_major))
    se_school = strip_period_parens(_compact_zh_spaces(se_school))
    se_major  = strip_period_parens(_compact_zh_spaces(se_major))

    return (degree_text, hi_school.strip(), hi_major.strip(), se_school.strip(), se_major.strip())

# ===== è§£æå–®ä¸€ HTML æª”ï¼ˆæŠ½æ¬„ä½ï¼‰ =====
def extract_fields_from_html(file_path, folder_label):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            html = f.read()
        soup = BeautifulSoup(html, "html.parser")
        full_text = soup.get_text(" ", strip=True)
        full_text = full_text.replace("\xa0", " ").replace("\u3000", " ")
        full_text = re.sub(r"[\s\u3000]+", " ", full_text).strip()
        tds = soup.find_all("td")
    except Exception as e:
        print(f"âŒ ç„¡æ³•è§£æ HTML æª”ï¼š{file_path} â†’ {e}")
        return None

    # åŸºæœ¬è³‡æ–™
    name, age, gender = "", "", ""
    m = re.search(r"((?:[A-Za-z]{2,20}\s*){1,5}|[\u4e00-\u9fa5]{2,4})\s*(\d{1,2})\s*æ­²\s+(ç”·|å¥³)", full_text)
    if m:
        name = m.group(1).strip()
        age = m.group(2)
        gender = m.group(3)
    print(f"âœ… æŠ“åˆ°ï¼šname='{name}', age='{age}', gender='{gender}'")

    # æ‡‰å¾µå¿«ç…§ / æ‡‰å¾µæ—¥æœŸ
    snapshot_match = re.search(r"æ‡‰å¾µå¿«ç…§[:ï¼š]?\s*(\d{4}[-/]\d{2}[-/]\d{2})\s*(\d{2}:\d{2})", full_text)
    apply_date_match = re.search(r"æ‡‰å¾µæ—¥æœŸ[:ï¼š]?\s*(\d{4}[-/]\d{2}[-/]\d{2})\s*(\d{2}:\d{2})", full_text)
    snapshot = pd.to_datetime(" ".join(snapshot_match.groups()), errors="coerce") if snapshot_match else pd.NaT
    apply_date = pd.to_datetime(" ".join(apply_date_match.groups()), errors="coerce") if apply_date_match else pd.NaT

    # æ‡‰å¾µè·å‹™èˆ‡ä»£ç¢¼
    job_match = re.search(r"æ‡‰å¾µè·å‹™[:ï¼š]?\s*(.{2,40}?)\s*(è‡ªæˆ‘æ¨è–¦|æ‡‰å¾µå•é¡Œ|å¸Œæœ›è·ç¨±|å·¥ä½œç¶“æ­·|å±…ä½åœ°|E-mail|è¯çµ¡é›»è©±|è¯çµ¡æ–¹å¼|$)", full_text)
    job = job_match.group(1).strip() if job_match else ""
    code_match = re.search(r"ä»£ç¢¼[:ï¼š]?\s*(\d{8,15})", full_text)
    code = code_match.group(1) if code_match else ""

    # æ•™è‚²èƒŒæ™¯ï¼ˆ**ç”¨æˆ‘å€‘çš„å‡½å¼**ï¼‰
    highest_degree, hi_school, hi_major, se_school, se_major = extract_education_fields_from_html(html)
    degree_bucket = degree_to_bucket(highest_degree)

    # ç¸½å¹´è³‡
    total_exp = ""
    for i in range(len(tds) - 1):
        label = tds[i].get_text(strip=True)
        value = tds[i + 1].get_text(strip=True)
        if label == "ç¸½å¹´è³‡":
            if "å¹´æ¬¡" in value:
                continue
            match = re.search(r"([0-9]{1,2}(?:~[0-9]{1,2})?å¹´(?:[ï¼ˆ(]?\s*å«\s*[ï¼‰)]?)?\s*(?:ä»¥ä¸Š|ä»¥ä¸‹)?)", value)
            total_exp = match.group(1) if match else value
            break

    if not total_exp:
        for tr in soup.find_all("tr"):
            th = tr.find("th")
            td = tr.find("td")
            if th and "ç¸½å¹´è³‡" in th.get_text(strip=True):
                value = td.get_text(strip=True)
                if "å¹´æ¬¡" in value:
                    continue
                match = re.search(r"([0-9]{1,2}(?:~[0-9]{1,2})?å¹´(?:[ï¼ˆ(]?\s*å«\s*[ï¼‰)]?)?\s*(?:ä»¥ä¸Š|ä»¥ä¸‹)?)", value)
                total_exp = match.group(1) if match else value
                break

    if not total_exp or re.search(r"ç„¡.*å·¥ä½œç¶“é©—", total_exp):
        total_exp = "ç„¡"

    # å·¥ä½œç¶“æ­·å¹´æ•¸
    work_exp = ""
    for i in range(len(tds) - 1):
        label = tds[i].get_text(strip=True)
        value = tds[i + 1].get_text(strip=True)
        if label == "å·¥ä½œç¶“æ­·":
            if "å¹´æ¬¡" in value:
                continue
            match = re.search(r"([0-9]{1,2}(?:~[0-9]{1,2})?å¹´(?:[ï¼ˆ(]?\s*å«\s*[ï¼‰)]?)?\s*(?:ä»¥ä¸Š|ä»¥ä¸‹)?)", value)
            work_exp = match.group(1) if match else value
            break

    if not work_exp:
        for tr in soup.find_all("tr"):
            th = tr.find("th")
            td = tr.find("td")
            if th and "å·¥ä½œç¶“æ­·" in th.get_text(strip=True):
                value = td.get_text(strip=True)
                if "å¹´æ¬¡" in value:
                    continue
                match = re.search(r"([0-9]{1,2}(?:~[0-9]{1,2})?å¹´(?:[ï¼ˆ(]?\s*å«\s*[ï¼‰)]?)?\s*(?:ä»¥ä¸Š|ä»¥ä¸‹)?)", value)
                work_exp = match.group(1) if match else value
                break

    if not work_exp or re.search(r"ç„¡.*å·¥ä½œç¶“é©—", work_exp):
        work_exp = "ç„¡"

    # â€”â€” èªæ–‡èƒ½åŠ› + è­‰ç…§æˆç¸¾ â€”â€” 
    lang_section = get_lang_section(full_text)   

    langs = [lang for lang in ["ä¸­æ–‡","è‹±æ–‡","æ—¥æ–‡","å°èª","å¾·æ–‡","æ³•æ–‡","è¥¿ç­ç‰™æ–‡","è¶Šæ–‡",
                            "æ³°æ–‡","éŸ“æ–‡","ç²µèª","å®¢èª","å…¶ä»–å¤–æ–‡"] if lang in lang_section]

    cert_scores = extract_cert_scores_from_lang(lang_section) 

    # å…¶ä»–æ¬„ä½
    address_match = re.search(r"å±…ä½åœ°[:ï¼š]?\s*(\d{3,5}\s*\S{3,}.*?)\s+(E-mail|è¯çµ¡é›»è©±|è¯çµ¡æ–¹å¼)", full_text)
    address = address_match.group(1).strip() if address_match else ""
    job_status_match = re.search(r"å°±æ¥­ç‹€æ…‹[:ï¼š]?\s*(ä»åœ¨è·|å¾…æ¥­ä¸­)", full_text)
    job_status = job_status_match.group(1) if job_status_match else ""

    # å¾…é‡
    salary_text, salary_number = "", ""
    salary_match = re.search(r"å¸Œæœ›å¾…é‡[:ï¼š]?\s*(.{1,80})", full_text)
    if salary_match:
        raw_salary = salary_match.group(1).strip()
        first_part = re.split(r"(å¯ä¸Šç­æ—¥|ä¸Šç­æ™‚æ®µ|å·¥ä½œç¶“æ­·|ç¸½å¹´è³‡|è¯çµ¡é›»è©±|è¯çµ¡æ–¹å¼|E-mail)", raw_salary)[0]
        number_match = re.search(r"\d{4,6}", first_part.replace(",", ""))
        if number_match:
            salary_number = number_match.group(0)
        if "é¢è­°" in first_part:
            salary_text = "é¢è­°"
        elif "ä¾å…¬å¸è¦å®š" in first_part:
            salary_text = "ä¾å…¬å¸è¦å®š"
        elif "æœˆè–ª" in first_part:
            salary_text = "æœˆè–ª"
        elif "æ™‚è–ª" in first_part:
            salary_text = "æ™‚è–ª"
        elif "å¹´è–ª" in first_part:
            salary_text = "å¹´è–ª"

    # é€£çµ¡è³‡è¨Š
    email = re.search(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+", full_text)
    phone = re.search(r"è¯çµ¡(é›»è©±|æ–¹å¼)[:ï¼š]?\s*(09\d{2}-?\d{3}-?\d{3})", full_text)

    # ä¸»æ—¨èˆ‡æ‡‰å¾µæ–¹å¼ä¾†æº
    subject_match = re.search(r"<!--subject:(.*?)-->", html)
    subject = subject_match.group(1).strip() if subject_match else ""
    apply_text = subject + " " + full_text
    if "ä¸»å‹•æ‡‰å¾µå±¥æ­·" in apply_text:
        apply_method = "ä¸»å‹•æ‡‰å¾µ"
    elif any(k in apply_text for k in ["é‚€è«‹æ‚¨æ‡‰å¾µ", "èª æ‘¯é‚€è«‹", "é‚€è«‹æ‚¨åŠ å…¥", "ä¸»å‹•é‚€è«‹"]):
        apply_method = "ä¼æ¥­é‚€ç´„"
    elif any(k in apply_text for k in ["å›è¦†é‚€è«‹", "æ¥å—é‚€è«‹", "å›è¦†é¢è©¦é‚€è«‹"]):
        apply_method = "å›è¦†é‚€ç´„"
    else:
        apply_method = "å…¶ä»–"

    source_channel = "104äººåŠ›éŠ€è¡Œ" if "104" in subject else "1111äººåŠ›éŠ€è¡Œ" if "1111" in subject else "å…¶ä»–"

    return {
        "è³‡æ–™å¤¾åç¨±": folder_label,
        "æ‡‰å¾µæ—¥æœŸ": snapshot.date() if not pd.isna(snapshot) else (apply_date.date() if not pd.isna(apply_date) else pd.NaT),
        "æ‡‰å¾µç®¡é“": source_channel,
        "æ‡‰å¾µæ–¹å¼": apply_method,
        "å§“å": name,
        "ä»£ç¢¼": code,
        "æ‡‰å¾µè·å‹™": job,
        "æ€§åˆ¥": gender,
        "å¹´é½¡": age,
        "å­¸æ­·": degree_bucket,
        "æœ€é«˜å­¸æ­·": highest_degree,
        "æœ€é«˜å­¸æ­·å­¸æ ¡": hi_school,
        "æœ€é«˜å­¸æ­·ç§‘ç³»": hi_major,
        "æ¬¡é«˜å­¸æ­·å­¸æ ¡": se_school,
        "æ¬¡é«˜å­¸æ­·ç§‘ç³»": se_major,
        "å·¥ä½œç¶“æ­·": work_exp,
        "ç¸½å¹´è³‡": total_exp,
        "å±…ä½åœ°": address,
        "å°±æ¥­ç‹€æ…‹": job_status,
        "å¸Œæœ›å¾…é‡": salary_text,
        "è–ªæ°´": salary_number,
        "æ‰‹æ©Ÿ": phone.group(2) if phone else "",
        "E-mail": email.group(0) if email else "",
        "èªè¨€èƒ½åŠ›": "ã€".join(langs),
        "è­‰ç…§æˆç¸¾": cert_scores
    }

# ===== æ‰¹æ¬¡è¼‰å…¥æª”æ¡ˆï¼ˆå…©è³‡æ–™å¤¾ï¼‰ =====
def load_all_resumes(folder, label):
    results = []
    failed = 0
    for fname in os.listdir(folder):
        if fname.lower().endswith(".html"):
            path = os.path.join(folder, fname)
            fields = extract_fields_from_html(path, label)
            if fields:
                results.append(fields)
            else:
                failed += 1
                print(f"âŒ ç„¡æ³•è§£æï¼š{fname}")
    print(f"ğŸ“‚ è³‡æ–™å¤¾ã€Œ{label}ã€å…±è¼‰å…¥ï¼šâœ… {len(results)} ç­†æˆåŠŸï¼ŒâŒ {failed} ç­†å¤±æ•—")
    return results

# ===== åŒ¯å‡º Excelï¼ˆä¿ç•™æœ€èˆŠï¼škeep='first'ï¼‰ =====
def update_excel_from_folder():

    fit = load_all_resumes(fit_folder, "åˆé©å±¥æ­·")
    unfit = load_all_resumes(unfit_folder, "ä¸åˆé©å±¥æ­·")
    all_new = fit + unfit

    if not all_new:
        print("âŒ æ²’æœ‰è®€åˆ°ä»»ä½•å±¥æ­·è³‡æ–™")
        return

    new_df = pd.DataFrame(all_new)
    new_df["æ‡‰å¾µæ—¥æœŸ"] = pd.to_datetime(new_df["æ‡‰å¾µæ—¥æœŸ"], errors="coerce")

    if os.path.exists(excel_path):
        old_df = pd.read_excel(excel_path)

        # â˜… èˆŠæ¬„ä½åŒæ­¥æ”¹åï¼Œé¿å…åŒæ™‚å‡ºç¾ã€Œå¤šç›Šæˆç¸¾ã€èˆ‡ã€Œè­‰ç…§æˆç¸¾ã€
        if "å¤šç›Šæˆç¸¾" in old_df.columns and "è­‰ç…§æˆç¸¾" not in old_df.columns:
            old_df = old_df.rename(columns={"å¤šç›Šæˆç¸¾": "è­‰ç…§æˆç¸¾"})

        old_df["æ‡‰å¾µæ—¥æœŸ"] = pd.to_datetime(old_df["æ‡‰å¾µæ—¥æœŸ"], errors="coerce")
    else:
        old_df = pd.DataFrame(columns=new_df.columns)

    combined = pd.concat([old_df, new_df], ignore_index=True)

    combined["ä»£ç¢¼"] = combined["ä»£ç¢¼"].astype(str).str.strip()
    combined["æ‡‰å¾µè·å‹™"] = combined["æ‡‰å¾µè·å‹™"].astype(str).str.replace(r"[\u3000\s]+", "", regex=True)

    try:
        with open("apply_count.pkl", "rb") as f:
            count_dict = pickle.load(f)
    except Exception:
        count_dict = defaultdict(int)

    combined.sort_values("æ‡‰å¾µæ—¥æœŸ", ascending=True, inplace=True)
    final_df = combined.drop_duplicates(subset=["ä»£ç¢¼", "æ‡‰å¾µè·å‹™"], keep="first").copy()

    final_df["æ‡‰å¾µæ¬¡æ•¸"] = final_df.apply(
        lambda r: str(count_dict.get((r["ä»£ç¢¼"], r["æ‡‰å¾µè·å‹™"]), 1)),
        axis=1
    )

    final_df = final_df.fillna("").astype(str)
    final_df["æ‡‰å¾µæ—¥æœŸ"] = pd.to_datetime(final_df["æ‡‰å¾µæ—¥æœŸ"], errors="coerce").dt.date
    final_df.sort_values("æ‡‰å¾µæ—¥æœŸ", ascending=True, inplace=True)

    final_df.to_excel(output_path, index=False)
    print(f"âœ… è¼¸å‡ºå®Œæˆï¼š{output_path}")

# ===== è®€å– Outlook ä¸¦åˆ†é¡ï¼ˆä¿ç•™æœ€èˆŠæª”ï¼‰ =====
def fetch_and_classify_emails():
    outlook = win32com.client.Dispatch("Outlook.Application").GetNamespace("MAPI")
    inbox = outlook.GetDefaultFolder(6)
    messages = inbox.Items
    messages.Sort("[ReceivedTime]", True)

    last_time = get_last_processed_time()
    restriction = f"[ReceivedTime] >= '{last_time.strftime('%m/%d/%Y %I:%M %p')}'"
    messages = messages.Restrict(restriction)
    apply_count_dict = defaultdict(int) 

    for msg in messages:
        if not hasattr(msg, "MessageClass") or msg.MessageClass != "IPM.Note":
            continue

        subject = msg.Subject or ""
        try:
            received_time = msg.ReceivedTime.replace(tzinfo=None)
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è®€å–æ™‚é–“ï¼š{subject} â†’ {e}")
            continue

        if not ("104æ‡‰å¾µå±¥æ­·" in subject or "104è½‰å¯„å±¥æ­·" in subject):
            continue
        if received_time <= last_time:
            print(f"â­ è·³éèˆŠä¿¡ä»¶ï¼š{subject}ï¼ˆ{received_time} <= {last_time}ï¼‰")
            continue

        try:
            html = msg.HTMLBody if hasattr(msg, "HTMLBody") else ""
            soup = BeautifulSoup(html, "html.parser")
            text = soup.get_text(" ", strip=True)
            fit = is_fit_candidate(text)

            folder = fit_folder if fit else unfit_folder
            safe_subject = re.sub(r"[\\/:*?\"<>|]", "_", subject).strip()
            file_name = f"{safe_subject}.html"
            save_path = os.path.join(folder, file_name)

            if "<!--subject:" not in html:
                html = f"<!--subject:{subject}-->" + html

            code_match = re.search(r"ä»£ç¢¼[:ï¼š]?\s*(\d{8,15})", text)
            job_match = re.search(r"æ‡‰å¾µè·å‹™[:ï¼š]?\s*(.{2,40}?)\s*(è‡ªæˆ‘æ¨è–¦|æ‡‰å¾µå•é¡Œ|å¸Œæœ›è·ç¨±|å·¥ä½œç¶“æ­·|å±…ä½åœ°|E-mail|è¯çµ¡é›»è©±|è¯çµ¡æ–¹å¼|$)", text)
            code = code_match.group(1) if code_match else ""
            job = job_match.group(1).strip() if job_match else ""
            apply_count_dict[(code, job)] += 1

            def extract_apply_time(txt):
                snapshot = re.search(r"æ‡‰å¾µå¿«ç…§[:ï¼š]?\s*(\d{4}[-/]\d{2}[-/]\d{2})\s*(\d{2}:\d{2})", txt)
                apply = re.search(r"æ‡‰å¾µæ—¥æœŸ[:ï¼š]?\s*(\d{4}[-/]\d{2}[-/]\d{2})\s*(\d{2}:\d{2})", txt)
                return pd.to_datetime(" ".join(snapshot.groups()), errors="coerce") if snapshot else (
                    pd.to_datetime(" ".join(apply.groups()), errors="coerce") if apply else pd.NaT
                )
            new_time = extract_apply_time(text)

            # å„²å­˜æˆ–æ›´æ–°ï¼ˆä¿ç•™æœ€èˆŠï¼‰
            if os.path.exists(save_path):
                with open(save_path, "r", encoding="utf-8") as f:
                    old_html = f.read()
                old_text = BeautifulSoup(old_html, "html.parser").get_text(" ", strip=True)
                old_time = extract_apply_time(old_text)

                # åªåœ¨æ–°é€²ä¾†çš„å±¥æ­·ã€Œæ›´èˆŠã€æ™‚æ‰è¦†è“‹ â†’ æ°¸é ä¿ç•™æœ€èˆŠçš„é‚£å°
                should_replace = False
                if pd.isna(old_time) and not pd.isna(new_time):
                    # èˆŠæª”çœ‹ä¸å‡ºæ™‚é–“ï¼Œä½†æ–°çš„æœ‰æ™‚é–“ â†’ è¦–æƒ…æ³ä¿ç•™èˆŠæˆ–æ›æˆæ›´èˆŠçš„ï¼›é€™è£¡ä¿å®ˆï¼šä¸æ›
                    should_replace = False
                elif not pd.isna(new_time) and not pd.isna(old_time):
                    should_replace = new_time < old_time
                else:
                    # å…©é‚Šéƒ½ NaT æˆ–æ–°çš„æ˜¯ NaT â†’ ä¸å‹•
                    should_replace = False

                if should_replace:
                    with open(save_path, "w", encoding="utf-8") as f:
                        f.write(html)
                    print(f"ğŸ” ä»¥æ›´èˆŠçš„å±¥æ­·è¦†è“‹ï¼š{file_name}ï¼ˆ{new_time} < {old_time}ï¼‰")
                else:
                    print(f"â­ ä¿ç•™æ—¢æœ‰å±¥æ­·ï¼ˆè¼ƒèˆŠç‰ˆæœ¬å„ªå…ˆï¼‰ï¼š{file_name}")
            else:
                with open(save_path, "w", encoding="utf-8") as f:
                    f.write(html)
                print(f"âœ… æˆåŠŸå„²å­˜ .html æª”ï¼š{file_name}")

        except Exception as e:
            print(f"âŒ éŒ¯èª¤è™•ç†ä¿¡ä»¶ï¼š{subject} â†’ {e}")
    save_current_time()

    with open("apply_count.pkl", "wb") as f:
        pickle.dump(apply_count_dict, f)

# ===== å…¥å£ =====
if __name__ == "__main__":
    fetch_and_classify_emails()
    update_excel_from_folder()
